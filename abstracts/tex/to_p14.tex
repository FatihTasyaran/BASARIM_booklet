
    \begin{abstract_online}{Sensitivity Analysis of Federated Learning over Decentralized Data and Communication Rounds}{%
        Mustafa Barış ÇAMLI, İsmail ARİ}{%
        }{%
        Ozyegin University - İstanbul, Turkey}
    Federated Learning (FL) refers to distributed learning via exchange of model metadata instead of raw data among the clients & servers in a centralized architecture or among peers in a decentralized architecture. It is quickly becoming the defacto standard in Machine Learning (ML) due to its network efficiency and privacy-preservation. However, there are several issues that need to be resolved, which require a sensitivity analysis of FL techniques to decentralized data and federation parameters. In this paper, we focus on federated learning with a client-server architecture. The clients train neural network (NN) models with their local data while the server takes weighted average of all models exchanged in periodic communication rounds. A Convolutional Neural Network (CNN) model is trained in a federated way over different image classification benchmark datasets. Our results demonstrate the effects of (1) datasets having independent & identically distributed (IID) vs. non-IID as well as balanced vs. unbalanced distributions, (2) number of communication rounds between clients and the server, and (3) the initial model selection in the distributed setting. In near future, we plan to extend our analysis to peer-to-peer architectures. 
    
        \textbf{Keywords} \newline{}Federated learning, neural networks, model training, communication round, IID, unbalanced, decentralized.
    \end{abstract_online}
    